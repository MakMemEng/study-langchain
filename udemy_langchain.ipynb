{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LSkqxoVN3J6"
      },
      "source": [
        "# Udemy講座「LangChainによる大規模言語モデル（LLM）アプリケーション開発入門」のコース前半のソースコード"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVs2pKa2ODmT"
      },
      "source": [
        "## 2. GPT の API の基礎知識"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLGNNT3BOWFp"
      },
      "source": [
        "### 2.4. OpenAI の API キーを発行"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0cuW-PeOD_s"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv(\"./.env\")\n",
        "os.environ.get(\"OPENAI_API_KEY\")\n",
        "# os.environ[\"OPENAI_API_KEY\"] = \"your-openai-api-key\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYVHgMnrOez3"
      },
      "source": [
        "### 2.5. OpenAI の Completions API にふれる"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cPPmp7IOGbU"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "url = \"https://api.openai.com/v1/completions\"\n",
        "headers = {\n",
        "    \"Content-Type\": \"application/json\",\n",
        "    \"Authorization\": \"Bearer \" + os.environ[\"OPENAI_API_KEY\"]\n",
        "}\n",
        "data = {\n",
        "    \"model\": \"text-davinci-003\",\n",
        "    \"prompt\": \"hello!\",\n",
        "    \"temperature\": 0,\n",
        "}\n",
        "\n",
        "response = requests.post(url=url, headers=headers, json=data)\n",
        "print(json.dumps(response.json(), indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WY7cd-EeOwiN"
      },
      "source": [
        "### 2.6. OpenAI の Chat API にふれる"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bOzgI4KGOIcj"
      },
      "outputs": [],
      "source": [
        "url = \"https://api.openai.com/v1/chat/completions\"\n",
        "headers = {\n",
        "    \"Content-Type\": \"application/json\",\n",
        "    \"Authorization\": \"Bearer \" + os.environ[\"OPENAI_API_KEY\"]\n",
        "}\n",
        "data = {\n",
        "    \"model\": \"gpt-3.5-turbo\",\n",
        "    \"messages\": [\n",
        "        {\"role\": \"user\", \"content\": \"hello!\"}\n",
        "    ],\n",
        "    \"temperature\": 0,\n",
        "}\n",
        "\n",
        "response = requests.post(url=url, headers=headers, json=data)\n",
        "print(json.dumps(response.json(), indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlDl6XqsOzFx"
      },
      "source": [
        "## 4. LangChain 入門（前半）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVO4MzQxPJDC"
      },
      "source": [
        "### 4.3. Models（LLMs・Chat Models）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lVxZFGSagE1C"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet langchain==0.0.172 openai==0.27.6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJHDUhwwgKc6"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm = OpenAI(model_name=\"text-davinci-003\", temperature=0)\n",
        "\n",
        "result = llm.predict(\"自己紹介してください。\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OShyvz70hRD2"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "result = chat.predict(\"自己紹介してください。\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9XTxsN8xitv"
      },
      "source": [
        "### 4.4. Prompts（Prompt Templates）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-sZIx1e8iwJ8"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "template = \"\"\"\n",
        "次のコマンドの概要を説明してください。\n",
        "\n",
        "コマンド: {command}\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"command\"],\n",
        "    template=template,\n",
        ")\n",
        "\n",
        "result = prompt.format(command=\"ls\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0eKtJGu0vX6"
      },
      "source": [
        "### 4.5. Chains"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QyYR_Iixyuy"
      },
      "outputs": [],
      "source": [
        "import langchain\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "langchain.verbose = True\n",
        "\n",
        "# Model を用意\n",
        "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "# Prompt を用意\n",
        "template = \"\"\"\n",
        "次のコマンドの概要を説明してください。\n",
        "\n",
        "コマンド: {command}\n",
        "\"\"\"\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"command\"],\n",
        "    template=template,\n",
        ")\n",
        "\n",
        "# Chain を作成\n",
        "chain = LLMChain(llm=chat, prompt=prompt)\n",
        "\n",
        "# 実行\n",
        "result = chain.run(\"ls\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BETIBsg104QZ"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain.chains import SimpleSequentialChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "langchain.verbose = True\n",
        "\n",
        "# Model を用意\n",
        "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "# 1 つ目の Prompt と Chain を用意\n",
        "cot_template = \"\"\"\n",
        "以下の質問に回答してください。\n",
        "\n",
        "### 質問 ###\n",
        "{question}\n",
        "### 質問終了 ###\n",
        "\n",
        "ステップバイステップで考えましょう。\n",
        "\"\"\"\n",
        "cot_prompt = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=cot_template,\n",
        ")\n",
        "cot_chain = LLMChain(llm=chat, prompt=cot_prompt)\n",
        "\n",
        "# 2 つ目の Prompt と Chain を用意\n",
        "summarize_template = \"\"\"\n",
        "入力を結論だけ一言に要約してください。\n",
        "\n",
        "### 入力 ###\n",
        "{input}\n",
        "### 入力終了 ###\n",
        "\"\"\"\n",
        "summarize_prompt = PromptTemplate(\n",
        "    input_variables=[\"input\"],\n",
        "    template=summarize_template,\n",
        ")\n",
        "summarize_chain = LLMChain(llm=chat, prompt=summarize_prompt)\n",
        "\n",
        "# 2 つの Chain を直列に繋ぐ\n",
        "cot_summarize_chain = SimpleSequentialChain(\n",
        "    chains=[cot_chain, summarize_chain])\n",
        "\n",
        "# 実行\n",
        "result = cot_summarize_chain(\n",
        "    \"私は市場に行って10個のリンゴを買いました。隣人に2つ、修理工に2つ渡しました。それから5つのリンゴを買って1つ食べました。残りは何個ですか？\")\n",
        "print(result[\"output\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1fNzjTRRl7u"
      },
      "source": [
        "### 4.7. Prompts のより高度な機能（Output Parses）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qkm0Ys9k4t3h"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from langchain.prompts import PromptTemplate\n",
        "from pydantic import BaseModel, Field, validator\n",
        "from typing import List\n",
        "\n",
        "langchain.verbose = True\n",
        "\n",
        "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "class Recipe(BaseModel):\n",
        "    ingredients: List[str] = Field(description=\"ingredients of the dish\")\n",
        "    steps: List[str] = Field(description=\"steps to make the dish\")\n",
        "\n",
        "template = \"\"\"料理のレシピを教えてください。\n",
        "\n",
        "{format_instructions}\n",
        "\n",
        "料理名: {dish}\n",
        "\"\"\"\n",
        "\n",
        "parser = PydanticOutputParser(pydantic_object=Recipe)\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"dish\"],\n",
        "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
        ")\n",
        "\n",
        "chain = LLMChain(llm=chat, prompt=prompt)\n",
        "\n",
        "output = chain.run(dish=\"カレー\")\n",
        "print(\"=== output ===\")\n",
        "print(output)\n",
        "\n",
        "recipe = parser.parse(output)\n",
        "print(\"=== recipe object ===\")\n",
        "print(recipe)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkfHWlKnSjSG"
      },
      "source": [
        "## 5. LangChain 入門（後半）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prFUg4G3QfbA"
      },
      "source": [
        "### 5.2. Indexes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VY_0R_VGcHv"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/hwchase17/langchain.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhcQzqUqR1t3"
      },
      "outputs": [],
      "source": [
        "!cd langchain && git checkout v0.0.172"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "znXiN1W1Rd8Q"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet unstructured==0.6.6 tabulate==0.9.0 pdf2image==1.16.3 pytesseract==0.3.10 chromadb==0.3.23 tiktoken==0.4.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q10Ff7vNQuV1"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import DirectoryLoader\n",
        "from langchain.indexes import VectorstoreIndexCreator\n",
        "\n",
        "loader = DirectoryLoader(\"./langchain/docs/\", glob=\"**/*.md\")\n",
        "index = VectorstoreIndexCreator().from_loaders([loader])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJbppn-lRblc"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "result = index.query(\"LangChainの概要を1文で説明してください。\", llm=chat)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARj8Y8_RdlD6"
      },
      "source": [
        "### 5.4. Memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7pxRwGnUDvn"
      },
      "outputs": [],
      "source": [
        "def post_chat_completions(content):\n",
        "  url = \"https://api.openai.com/v1/chat/completions\"\n",
        "  headers = {\n",
        "      \"Content-Type\": \"application/json\",\n",
        "      \"Authorization\": \"Bearer \" + os.environ[\"OPENAI_API_KEY\"]\n",
        "  }\n",
        "  data = {\n",
        "      \"model\": \"gpt-3.5-turbo\",\n",
        "      \"messages\": [\n",
        "          {\"role\": \"user\", \"content\": content}\n",
        "      ],\n",
        "      \"temperature\": 0,\n",
        "  }\n",
        "\n",
        "  response = requests.post(url=url, headers=headers, json=data)\n",
        "  print(json.dumps(response.json(), indent=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_OBzUQGVAwa"
      },
      "outputs": [],
      "source": [
        "post_chat_completions(\"Hi! I'm Oshima!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EYjoBKp8VAjZ"
      },
      "outputs": [],
      "source": [
        "post_chat_completions(\"Do you know my name?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ctwcsZ-VLdD"
      },
      "outputs": [],
      "source": [
        "post_chat_completions(\"\"\"Human: Hi! I'm Oshima!\n",
        "AI: Hello Oshima! I'm an AI language model. How can I assist you today?\n",
        "Human: Do you know my name?\n",
        "AI: \"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjkhKO7nVeEK"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import ConversationChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "\n",
        "langchain.verbose = True\n",
        "\n",
        "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0, max_tokens=100)\n",
        "conversation = ConversationChain(\n",
        "    llm=chat,\n",
        "    memory=ConversationBufferMemory()\n",
        ")\n",
        "\n",
        "while True:\n",
        "    user_message = input(\"You: \")\n",
        "    ai_message = conversation.predict(input=user_message)\n",
        "    print(f\"AI: {ai_message}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0xTWbzzV4_n"
      },
      "source": [
        "### 5.5. Agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbSTOTuKX7kJ"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import load_tools\n",
        "from langchain.agents import initialize_agent\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "langchain.verbose = True\n",
        "\n",
        "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "tools = load_tools([\"terminal\"], llm=chat)\n",
        "agent_chain = initialize_agent(\n",
        "    tools, chat, agent=\"zero-shot-react-description\")\n",
        "\n",
        "result = agent_chain.run(\"What is your current directory?\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqrKxwnhlstV"
      },
      "source": [
        "### 5.8. Chat API に特化した実装"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jX1cLM9FJfbe"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import ConversationChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "import openai\n",
        "\n",
        "langchain.verbose = True\n",
        "openai.log = \"debug\"\n",
        "\n",
        "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0, max_tokens=100)\n",
        "conversation = ConversationChain(\n",
        "    llm=chat,\n",
        "    memory=ConversationBufferMemory()\n",
        ")\n",
        "\n",
        "while True:\n",
        "    user_message = input(\"You: \")\n",
        "    ai_message = conversation.predict(input=user_message)\n",
        "    print(f\"AI: {ai_message}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X90oA5kIPGYG"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import (\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage\n",
        ")\n",
        "\n",
        "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0, max_tokens=100)\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
        "    HumanMessage(content=\"Hi! I'm Oshima!\")\n",
        "]\n",
        "\n",
        "result = chat(messages)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZAE9dogmna_"
      },
      "outputs": [],
      "source": [
        "langchain.verbose = True\n",
        "\n",
        "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "memory = ConversationBufferMemory()\n",
        "\n",
        "while True:\n",
        "    user_message = input(\"You: \")\n",
        "    memory.chat_memory.add_user_message(user_message)\n",
        "\n",
        "    ai_message = chat(memory.chat_memory.messages)\n",
        "    memory.chat_memory.add_ai_message(ai_message.content)\n",
        "    print(f\"AI: {ai_message.content}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyhA5HVJLGI-"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
